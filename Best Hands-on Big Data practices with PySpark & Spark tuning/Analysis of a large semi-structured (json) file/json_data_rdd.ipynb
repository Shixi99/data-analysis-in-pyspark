{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "    .setMaster('local[*]')\\\n",
    "    .setAppName('Json analysis with rdd')\\\n",
    "    .setExecutorEnv('spark.driver.memory','4g')\\\n",
    "    .setExecutorEnv('spark.executor.memory','4g')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data source: https://www.kaggle.com/datasets/Cornell-University/arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_json = sc.textFile('arxiv-metadata-oai-snapshot.json', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd_json.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[62] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# setting storage level to MEMORY_AND_DISK\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0704.0001',\n",
       "  'submitter': 'Pavel Nadolsky',\n",
       "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       "  'comments': '37 pages, 15 figures; published version',\n",
       "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       "  'doi': '10.1103/PhysRevD.76.013009',\n",
       "  'report-no': 'ANL-HEP-PR-07-12',\n",
       "  'categories': 'hep-ph',\n",
       "  'license': None,\n",
       "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       "  'update_date': '2008-11-26',\n",
       "  'authors_parsed': [['Balázs', 'C.', ''],\n",
       "   ['Berger', 'E. L.', ''],\n",
       "   ['Nadolsky', 'P. M.', ''],\n",
       "   ['Yuan', 'C. -P.', '']]}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 100\n",
      "Default paralellism:  16\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of partitions: {rdd.getNumPartitions()}')\n",
    "print(f'Default paralellism:  {sc.defaultParallelism}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011231"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the first 2 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0704.0001',\n",
       "  'submitter': 'Pavel Nadolsky',\n",
       "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       "  'comments': '37 pages, 15 figures; published version',\n",
       "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       "  'doi': '10.1103/PhysRevD.76.013009',\n",
       "  'report-no': 'ANL-HEP-PR-07-12',\n",
       "  'categories': 'hep-ph',\n",
       "  'license': None,\n",
       "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       "  'update_date': '2008-11-26',\n",
       "  'authors_parsed': [['Balázs', 'C.', ''],\n",
       "   ['Berger', 'E. L.', ''],\n",
       "   ['Nadolsky', 'P. M.', ''],\n",
       "   ['Yuan', 'C. -P.', '']]},\n",
       " {'id': '0704.0002',\n",
       "  'submitter': 'Louis Theran',\n",
       "  'authors': 'Ileana Streinu and Louis Theran',\n",
       "  'title': 'Sparsity-certifying Graph Decompositions',\n",
       "  'comments': 'To appear in Graphs and Combinatorics',\n",
       "  'journal-ref': None,\n",
       "  'doi': None,\n",
       "  'report-no': None,\n",
       "  'categories': 'math.CO cs.CG',\n",
       "  'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n",
       "  'abstract': '  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'},\n",
       "   {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}],\n",
       "  'update_date': '2008-12-13',\n",
       "  'authors_parsed': [['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']]}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['authors', 'comments', 'title', 'id', 'journal-ref', 'versions', 'submitter', 'categories', 'update_date', 'authors_parsed', 'report-no', 'license', 'abstract', 'doi']\n"
     ]
    }
   ],
   "source": [
    "attributes = rdd.flatMap(lambda x: x.keys()).distinct().collect()\n",
    "print(attributes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get the name of the license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'http://creativecommons.org/licenses/publicdomain/', 'http://creativecommons.org/licenses/by-nc-nd/4.0/', 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'http://creativecommons.org/licenses/by-nc-sa/3.0/', 'http://creativecommons.org/licenses/by/3.0/', 'http://creativecommons.org/licenses/by/4.0/', 'http://creativecommons.org/publicdomain/zero/1.0/', 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'http://creativecommons.org/licenses/by-sa/4.0/']\n"
     ]
    }
   ],
   "source": [
    "licenses = rdd.map(lambda x: x['license']).distinct().collect()\n",
    "print(licenses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The shortest and longest titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest title : !-Graphs with Trivial Overlap are Context-Free\n",
      "Longest title : Weyl formula for the negative dissipative eigenvalues of Maxwell's\n",
      "  equations\n"
     ]
    }
   ],
   "source": [
    "shortest_title = rdd.map(lambda x: x['title']).reduce(lambda x,y: x if x < y else y)\n",
    "longest_title =  rdd.map(lambda x: x['title']).reduce(lambda x,y: x if x > y else y)\n",
    "\n",
    "print('Shortest title :', shortest_title)\n",
    "print('Longest title :', longest_title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find abbrevations with 5 or more letters in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_abbrivations(line):\n",
    "    result = re.search(r\"\\(([A-Z][^!@#/+-_<>.,$%())]{5,})\\)\", line)\n",
    "    if result:\n",
    "        return result.group(1)\n",
    "\n",
    "abbv_rdd = rdd.filter(lambda x: get_abbrivations(x['abstract']))\n",
    "abbv_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0704.0261',\n",
       "  'submitter': 'Scott Randall',\n",
       "  'authors': 'Scott W. Randall, Maxim Markevitch, Douglas Clowe, Anthony H.\\n  Gonzalez, and Marusa Bradac',\n",
       "  'title': 'Constraints on the Self-Interaction Cross-Section of Dark Matter from\\n  Numerical Simulations of the Merging Galaxy Cluster 1E 0657-5',\n",
       "  'comments': None,\n",
       "  'journal-ref': None,\n",
       "  'doi': '10.1086/587859',\n",
       "  'report-no': None,\n",
       "  'categories': 'astro-ph',\n",
       "  'license': None,\n",
       "  'abstract': \"  (Abridged) We compare recent results from X-ray, strong lensing, weak\\nlensing, and optical observations with numerical simulations of the merging\\ngalaxy cluster 1E0657-56. X-ray observations reveal a bullet-like subcluster\\nwith a prominent bow shock, while lensing results show that the positions of\\nthe total mass peaks are consistent with the centroids of the collisionless\\ngalaxies (and inconsistent with the X-ray brightness peaks). Previous studies,\\nbased on older observational datasets, have placed upper limits on the\\nself-interaction cross-section of dark matter per unit mass, sigma/m, using\\nsimplified analytic techniques. In this work, we take advantage of new,\\nhigher-quality observational datasets by running N-body simulations of\\n1E0657-56 that include the effects of self-interacting dark matter, and\\ncomparing the results with observations. Furthermore, the recent data allow for\\na new independent method of constraining sigma/m, based on the non-observation\\nof an offset between the bullet subcluster mass peak and galaxy centroid. This\\nnew method places an upper limit (68% confidence) of sigma/m < 1.25 cm^2/g. If\\nwe make the assumption that the subcluster and the main cluster had equal\\nmass-to-light ratios prior to the merger, we derive our most stringent\\nconstraint of sigma/m < 0.7 cm^2/g, which comes from the consistency of the\\nsubcluster's observed mass-to-light ratio with the main cluster's, and with the\\nuniversal cluster value, ruling out the possibility of a large fraction of dark\\nmatter particles being scattered away due to collisions. Our limit is a slight\\nimprovement over the previous result from analytic estimates, and rules out\\nmost of the 0.5 - 5cm^2/g range invoked to explain inconsistencies between the\\nstandard collisionless cold dark matter model and observations.\\n\",\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 20:48:02 GMT'}],\n",
       "  'update_date': '2009-11-13',\n",
       "  'authors_parsed': [['Randall', 'Scott W.', ''],\n",
       "   ['Markevitch', 'Maxim', ''],\n",
       "   ['Clowe', 'Douglas', ''],\n",
       "   ['Gonzalez', 'Anthony H.', ''],\n",
       "   ['Bradac', 'Marusa', '']]}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbv_rdd.take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get the number of archive reports per months (update_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_date(DateIn):\n",
    "    return datetime.strptime(DateIn, '%Y-%m-%d').month\n",
    "\n",
    "extract_date('2004-05-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 134247),\n",
       " (2, 116948),\n",
       " (3, 126458),\n",
       " (4, 117126),\n",
       " (5, 296587),\n",
       " (6, 191746),\n",
       " (7, 122649),\n",
       " (8, 138469),\n",
       " (9, 138978),\n",
       " (10, 197755),\n",
       " (11, 297963),\n",
       " (12, 132305)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (extract_date(x['update_date']), 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 116948),\n",
       " (4, 117126),\n",
       " (7, 122649),\n",
       " (3, 126458),\n",
       " (12, 132305),\n",
       " (1, 134247),\n",
       " (8, 138469),\n",
       " (9, 138978),\n",
       " (6, 191746),\n",
       " (10, 197755),\n",
       " (5, 296587),\n",
       " (11, 297963)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (extract_date(x['update_date']), 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortBy(lambda x: x[1])\\\n",
    "    .collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Get average number of pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_page(line):\n",
    "    search = re.findall('\\d+ pages', line)\n",
    "    if search:\n",
    "        return int(search[0].split(' ')[0])\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "get_page('123 pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1184075 \n",
      "Sum: 21139516 \n",
      "Average: 18\n"
     ]
    }
   ],
   "source": [
    "rdd_avg = rdd.map(lambda x: get_page(x['comments'] if x['comments'] is not None else 'None'))\n",
    "\n",
    "# remove 0's\n",
    "rdd_avg = rdd_avg.filter(lambda x: x!=0)\n",
    "counter = rdd_avg.count()\n",
    "summ = rdd_avg.reduce(lambda x,y: x+y)\n",
    "\n",
    "print(f'Count: {counter} \\nSum: {summ} \\nAverage: {round(summ / counter)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max page: 11232 \n",
      "Min page: 1\n"
     ]
    }
   ],
   "source": [
    "max_page = rdd_avg.reduce(lambda x,y: x if x>y else y)\n",
    "min_page = rdd_avg.reduce(lambda x,y: x if x<y else y)\n",
    "\n",
    "print(f'Max page: {max_page} \\nMin page: {min_page}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b785a0a06e2d7a312a57a7dba8bbee97b75bb74680ea935274a48dc11f29425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
